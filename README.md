# spark_practice 

> A local PySpark practice environment using Docker, Apache Spark, MinIO (S3-compatible storage), and Jupyter Lab.

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Docker Network                        â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ Spark Master â”‚â—„â”€â”€â”€â”‚  Worker 1  â”‚    â”‚  Worker 2  â”‚     â”‚
â”‚   â”‚  :8080 :7077 â”‚    â”‚   :8081    â”‚    â”‚   :8082    â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚          â”‚                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   Jupyter    â”‚    â”‚     MinIO  (S3-compatible)      â”‚  â”‚
â”‚   â”‚    :8888     â”‚    â”‚   API :9000  |  Console :9001   â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‚ Project Structure

```
spark_practice/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ ingest_to_minio.py        # Ingest CSVs â†’ Parquet â†’ MinIO
â”œâ”€â”€ config/
â”‚   â””â”€â”€ spark-defaults.conf       # Spark + S3A/MinIO configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ employees.csv             # 150 rows â€” employee records
â”‚   â””â”€â”€ sales.csv                 # 200 rows â€” sales transactions
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ practice.ipynb            # Interactive Jupyter notebook
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_basic_transformations.py
â”‚   â”œâ”€â”€ 02_aggregations.py
â”‚   â”œâ”€â”€ 03_joins.py
â”‚   â”œâ”€â”€ 04_window_functions.py
â”‚   â”œâ”€â”€ 05_minio_integration.py
â”‚   â””â”€â”€ 06_udf_and_sql.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸš€ Quick Start

### Prerequisites
- Docker Desktop (with at least 8GB RAM allocated)
- Git

### 1. Clone the repo
```bash
git clone https://github.com/Saptarshi0/spark_practice.git
cd spark_practice
```

### 2. Start the cluster
```bash
docker compose up -d
```
Wait ~30 seconds for all services to be healthy.

### 3. Check all services are running
```bash
docker compose ps
```

| Service | URL | Credentials |
|---------|-----|-------------|
| Spark Master UI | http://localhost:8080 | â€” |
| Spark Worker 1 | http://localhost:8081 | â€” |
| Spark Worker 2 | http://localhost:8082 | â€” |
| Jupyter Lab | http://localhost:8888 | `docker logs jupyter 2>&1 \| grep token` |
| MinIO Console | http://localhost:9001 | `minioadmin` / `minioadmin` |

---

## ðŸ“¥ Step 1 â€” Ingest Data into MinIO

Run the ingest job to load CSV files â†’ Parquet format â†’ MinIO:

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
    --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
    /opt/spark/work/app/ingest_to_minio.py
```

This will:
- Read `employees.csv` (150 rows) and `sales.csv` (200 rows) from the `data/` folder
- Convert both to **Parquet format** (columnar, compressed with Snappy)
- Write to MinIO â€” `employees/` partitioned by `department`, `sales/` partitioned by `region`
- Verify the write by reading back from MinIO

After running, open http://localhost:9001 â†’ **spark-data** bucket to see the Parquet files.

---

## â–¶ï¸ Step 2 â€” Run Practice Scripts

```bash
# 01 â€” Basic Transformations
docker exec spark-master /opt/spark/bin/spark-submit \
    /opt/spark/work/scripts/01_basic_transformations.py

# 02 â€” Aggregations
docker exec spark-master /opt/spark/bin/spark-submit \
    /opt/spark/work/scripts/02_aggregations.py

# 03 â€” All Join Types
docker exec spark-master /opt/spark/bin/spark-submit \
    /opt/spark/work/scripts/03_joins.py

# 04 â€” Window Functions
docker exec spark-master /opt/spark/bin/spark-submit \
    /opt/spark/work/scripts/04_window_functions.py

# 05 â€” MinIO Read/Write
docker exec spark-master /opt/spark/bin/spark-submit \
    --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
    /opt/spark/work/scripts/05_minio_integration.py

# 06 â€” UDFs + Spark SQL
docker exec spark-master /opt/spark/bin/spark-submit \
    /opt/spark/work/scripts/06_udf_and_sql.py
```

---

## ðŸ““ Jupyter Notebook

1. Get the token: `docker logs jupyter 2>&1 | grep token`
2. Open http://localhost:8888 and paste the token
3. Navigate to `work/notebooks/practice.ipynb`
4. The notebook connects directly to the Spark cluster

---

## ðŸ“Š Datasets

### employees.csv (150 rows)
| Column | Type | Description |
|--------|------|-------------|
| employee_id | int | Unique ID |
| name | string | Full name |
| department | string | Engineering / Marketing / Finance / HR |
| salary | int | Annual salary |
| hire_date | date | Date joined |
| city | string | New York / SF / Chicago / Austin |
| age | int | Age |
| gender | string | M / F |
| experience_years | int | Years of experience |
| performance_score | float | 3.3 â€“ 5.0 |
| manager_id | int | ID of their manager |

### sales.csv (200 rows)
| Column | Type | Description |
|--------|------|-------------|
| sale_id | int | Unique ID |
| employee_id | int | FK to employees |
| product | string | Laptop / Phone / Tablet etc. |
| amount | float | Sale value |
| sale_date | date | Date of sale |
| region | string | North / South / East / West / Central |
| units | int | Units sold |
| discount | float | Discount applied (0â€“0.2) |
| channel | string | Online / In-Store / Partner / Direct |

---

## ðŸ“š Topics Covered

| Script | Topics |
|--------|--------|
| `ingest_to_minio.py` | SparkSession config, CSV â†’ Parquet, S3A/MinIO write, partitioning |
| `01_basic_transformations.py` | `select`, `filter`, `withColumn`, `cast`, `drop`, `distinct`, `dropDuplicates` |
| `02_aggregations.py` | `groupBy`, `agg`, `pivot`, `rollup`, `cube`, `collect_list` |
| `03_joins.py` | inner / left / right / full / semi / anti / broadcast joins |
| `04_window_functions.py` | `rank`, `dense_rank`, `row_number`, `lag`, `lead`, running totals, `ntile`, `percent_rank` |
| `05_minio_integration.py` | Read/write CSV & Parquet, partitioned writes, MinIO (S3A) |
| `06_udf_and_sql.py` | Python UDFs, Pandas (vectorised) UDFs, `createOrReplaceTempView`, Spark SQL |

---

## ðŸ›‘ Stop the Cluster

```bash
# Stop but keep data
docker compose stop

# Stop and remove everything including volumes
docker compose down -v
```

---


## ðŸ”§ Tech Stack

| Tool | Version | Purpose |
|------|---------|---------|
| Apache Spark | 3.5.1 | Distributed data processing |
| PySpark | 3.5.1 | Python API for Spark |
| MinIO | latest | Local S3-compatible object storage |
| Jupyter Lab | latest | Interactive notebook environment |
| Docker | â€” | Container orchestration |

---

*Happy Sparking! ðŸš€*