# spark_practice

> PySpark practice project using a local Docker Spark cluster + MinIO (S3-compatible storage).

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network                   â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Spark Master â”‚â—„â”€â”€â”€â”‚ Worker 1 â”‚  â”‚ Worker 2 â”‚    â”‚
â”‚  â”‚  :8080 :7077 â”‚    â”‚  :8081   â”‚  â”‚  :8082   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Jupyter    â”‚    â”‚  MinIO (S3-compatible)   â”‚  â”‚
â”‚  â”‚    :8888     â”‚    â”‚  API :9000 | UI :9001    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. Clone the repo
```bash
git clone https://github.com/Saptarshi0/spark_practice.git
cd spark_practice
```

### 2. Start the cluster
```bash
docker compose up -d
```
Wait ~30 seconds for all services to be healthy.

### 3. Verify everything is running
| Service | URL |
|---------|-----|
| Spark Master UI | http://localhost:8080 |
| Spark Worker 1 | http://localhost:8081 |
| Spark Worker 2 | http://localhost:8082 |
| Jupyter Lab | http://localhost:8888 (token: check logs) |
| MinIO Console | http://localhost:9001 (user: `minioadmin` / pw: `minioadmin`) |

Get the Jupyter token:
```bash
docker logs jupyter 2>&1 | grep token
```

---

## ğŸ“‚ Project Structure

```
spark_practice/
â”œâ”€â”€ docker-compose.yml          # Full cluster definition
â”œâ”€â”€ requirements.txt            # Local Python packages
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config/
â”‚   â””â”€â”€ spark-defaults.conf    # Spark + S3A/MinIO settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ employees.csv          # Sample employee data (20 rows)
â”‚   â””â”€â”€ sales.csv              # Sample sales data (20 rows)
â”œâ”€â”€ scripts/                   # Run via spark-submit inside Docker
â”‚   â”œâ”€â”€ 01_basic_transformations.py   # select, filter, withColumn, cast
â”‚   â”œâ”€â”€ 02_aggregations.py            # groupBy, agg, pivot, rollup, cube
â”‚   â”œâ”€â”€ 03_joins.py                   # inner/left/right/full/semi/anti/broadcast
â”‚   â”œâ”€â”€ 04_window_functions.py        # rank, lag/lead, running totals, ntile
â”‚   â”œâ”€â”€ 05_minio_integration.py       # read/write Parquet & CSV to MinIO
â”‚   â””â”€â”€ 06_udf_and_sql.py             # Python UDFs, Pandas UDFs, Spark SQL
â””â”€â”€ notebooks/
    â””â”€â”€ practice.ipynb         # Interactive Jupyter notebook
```

---

## â–¶ï¸ Running Scripts

All scripts run inside the `spark-master` container:

```bash
# Basic transformations
docker exec spark-master spark-submit \
    /opt/bitnami/spark/work/scripts/01_basic_transformations.py

# Aggregations
docker exec spark-master spark-submit \
    /opt/bitnami/spark/work/scripts/02_aggregations.py

# Joins
docker exec spark-master spark-submit \
    /opt/bitnami/spark/work/scripts/03_joins.py

# Window functions
docker exec spark-master spark-submit \
    /opt/bitnami/spark/work/scripts/04_window_functions.py

# MinIO read/write (needs hadoop-aws packages)
docker exec spark-master spark-submit \
    --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
    /opt/bitnami/spark/work/scripts/05_minio_integration.py

# UDFs + Spark SQL
docker exec spark-master spark-submit \
    /opt/bitnami/spark/work/scripts/06_udf_and_sql.py
```

---

## ğŸ““ Jupyter Notebook

1. Open http://localhost:8888
2. Navigate to `work/notebooks/practice.ipynb`
3. Run all cells â€” the notebook connects directly to the Spark cluster

---

## ğŸ›‘ Stop the Cluster

```bash
# Stop (keep data)
docker compose stop

# Stop + remove containers + volumes
docker compose down -v
```

---

## ğŸ“š Topics Covered

| Script | Topics |
|--------|--------|
| 01 | `select`, `filter`, `withColumn`, `cast`, `drop`, `distinct`, `dropDuplicates` |
| 02 | `groupBy`, `agg`, `pivot`, `rollup`, `cube`, `collect_list` |
| 03 | inner / left / right / full / semi / anti / broadcast joins |
| 04 | `rank`, `dense_rank`, `row_number`, `lag`, `lead`, running totals, `ntile`, `percent_rank` |
| 05 | Read/write CSV & Parquet, partitioned writes, MinIO (S3A) integration |
| 06 | Python UDFs, Pandas (vectorised) UDFs, `createOrReplaceTempView`, Spark SQL |

---

## âš™ï¸ Requirements

- Docker Desktop â‰¥ 4.x
- 6 GB RAM available for Docker (4 containers)
- No local Spark / Java installation needed

---

*Happy Sparking! ğŸš€*